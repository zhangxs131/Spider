{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "卡内基spider.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN07HVpW5yBTmgZYytqTFTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/Spider/blob/main/%E5%8D%A1%E5%86%85%E5%9F%BAspider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#卡内基spider \n",
        "\n",
        "网站  https://carnegieendowment.org/ 卡内基国际和平基金会 \n",
        "\n",
        "爬取只能用selenium从首页进入，因为直接通过url访问搜索页面会被拒绝"
      ],
      "metadata": {
        "id": "_t8WBjKlYPum"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3Btw90CYBSE",
        "outputId": "97c23132-d6b7-4e79-b674-4ff557236c4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,628 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,067 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,478 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,256 kB]\n",
            "Fetched 9,685 kB in 3s (2,896 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 88.3 MB of archives.\n",
            "After this operation, 294 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 99.0.4844.51-0ubuntu0.18.04.1 [1,143 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 99.0.4844.51-0ubuntu0.18.04.1 [77.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 99.0.4844.51-0ubuntu0.18.04.1 [4,388 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 99.0.4844.51-0ubuntu0.18.04.1 [5,092 kB]\n",
            "Fetched 88.3 MB in 4s (19.6 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_99.0.4844.51-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
            "\u001b[K     |████████████████████████████████| 968 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 59.0 MB/s \n",
            "\u001b[?25hCollecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 48.5 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n"
          ]
        }
      ],
      "source": [
        "#selenium 和 driver安装\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords=['Ukraine','Russia']\n",
        "max_pages=10\n",
        "\n",
        "ori_url='https://carnegieendowment.org'\n",
        "start_time='2022-03-09 23:00:00'\n",
        "end_time='2022-03-13 00:00:01'"
      ],
      "metadata": {
        "id": "O-jp9R8raICc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from lxml import etree\n",
        "\n",
        "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
        "                        'Chrome/51.0.2704.63 Safari/537.36'}\n",
        "\n",
        "def get_sampler(sample):\n",
        "  try:\n",
        "    page_res = requests.get(sample['url'], headers = headers, timeout = 30)\n",
        "    page_res.raise_for_status()\n",
        "    page_res.encoding = page_res.apparent_encoding\n",
        "\n",
        "  except:\n",
        "    print('error in geting text pages')\n",
        "    return {}\n",
        "  t=etree.HTML(page_res.text)\n",
        "  try:\n",
        "    sample['text']=(' ').join(t.xpath('//div[@class=\"article-body\"]/p//text()'))\n",
        "  except:\n",
        "    return {}\n",
        "  return sample"
      ],
      "metadata": {
        "id": "t_jlIVHWmy4d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "#配置option\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "browser= webdriver.Chrome(options=options)\n",
        "\n",
        "#存储数据\n",
        "result={}\n",
        "sum=0\n",
        "\n",
        "# 搜索不同关键词\n",
        "for keyword in keywords:\n",
        "  print('__________under keyword {}______________'.format(keyword))\n",
        "\n",
        "  data=[]\n",
        "  browser.get(\"https://carnegieendowment.org/\")\n",
        "\n",
        "  time.sleep(2)\n",
        "  print(browser.title)\n",
        "\n",
        "  #点击搜索框\n",
        "  search_a=browser.find_element(by=By.XPATH,value='//a[@class=\"menu-trigger main-search-trigger header-navigation-trigger\"]')\n",
        "  search_a.click()\n",
        "\n",
        "  #选择搜索框并输入关键词\n",
        "  WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//input[@name=\"qry\"]')))\n",
        "  input=browser.find_element(by=By.XPATH,value='//input[@name=\"qry\"]')\n",
        "  input.send_keys(keyword)\n",
        "  input.send_keys(Keys.ENTER)\n",
        "\n",
        "  time.sleep(3)\n",
        "  print(browser.title)\n",
        "  print('search_url',browser.current_url)\n",
        "\n",
        "  #选择按时间排序\n",
        "  WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//ul[@class=\"nav nav-tabs control-tabs\"]/li')))\n",
        "  t_datasort=browser.find_elements(by=By.XPATH,value='//ul[@class=\"nav nav-tabs control-tabs\"]/li')\n",
        "  datasort=t_datasort[0].find_element(by=By.XPATH,value='./a')\n",
        "  datasort.click()\n",
        "\n",
        "\n",
        "\n",
        "  #选择文章类型\n",
        "  WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//a[@class=\"accordion-trigger--alt arrow-down-link border-radius\"]')))\n",
        "  print('date_sort_search_url',browser.current_url)\n",
        "  document_type=browser.find_elements(by=By.XPATH,value='//a[@class=\"accordion-trigger--alt arrow-down-link border-radius\"]')[0]\n",
        "  document_type.click()\n",
        "\n",
        "  #这里只选择了article类型，需要增加其他类型文章，并编写解析函数，否则无数据\n",
        "  WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//li[@class=\"search-result-filters__filter\"]')))\n",
        "  article_type=browser.find_elements(by=By.XPATH,value='//li[@class=\"search-result-filters__filter\"]')[0].find_element(by=By.XPATH,value='./a')\n",
        "  article_type.click()\n",
        "  time.sleep(2)\n",
        "\n",
        "  #获取文章list clearfix\n",
        "\n",
        "  #不同页数\n",
        "  for i in range(max_pages):\n",
        "    WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//li[@class=\"clearfix\"]')))\n",
        "    lists=browser.find_elements(by=By.XPATH,value='//li[@class=\"clearfix\"]')\n",
        "\n",
        "    if len(lists)==0:\n",
        "      break\n",
        "    sample={}\n",
        "    for j in lists:\n",
        "      #获取信息\n",
        "\n",
        "      try:\n",
        "        url_temp=j.find_element(by=By.XPATH,value='./h4/a').get_attribute('href')\n",
        "        if not url_temp.startswith('http'):\n",
        "          sample['url']=ori_url+url_temp\n",
        "        else:\n",
        "          sample['url']=url_temp\n",
        "        \n",
        "        sample['title']=j.find_element(by=By.XPATH,value='./h4/a').text\n",
        "        time2=j.find_element(by=By.XPATH,value='.//div[@class=\"gutter-bottom-narrow meta sans-serif uppercase smallest-text tighten-line-height\"]').text\n",
        "        time_temp=(' ').join(time2.split(' ')[-3:])\n",
        "        sample['time']=str(datetime.datetime.strptime(time_temp,'%B %d, %Y'))\n",
        "\n",
        "        #判断时间\n",
        "        if sample['time']<start_time:\n",
        "          break\n",
        "\n",
        "        sample=get_sampler(sample)\n",
        "        if sample=={}:\n",
        "          continue\n",
        "        data.append(sample)\n",
        "        sum+=1\n",
        "        if sum%5 ==0:\n",
        "          print('第{}条数据 时间 {}，标题 {}'.format(sum,sample['time'],sample['title']))\n",
        "      except:\n",
        "        continue\n",
        "    \n",
        "    \n",
        "    #比较时间\n",
        "    if sample['time']<start_time:\n",
        "      print('爬取数据到达{} ，爬取结束'.format(sample['time']))\n",
        "      break\n",
        "\n",
        "    #跳到下一页\n",
        "    try:\n",
        "      page_label=browser.find_elements(by=By.XPATH,value='//a[@class=\"page-link\"]')[i]\n",
        "      page_label.click()\n",
        "    except:\n",
        "      print('无法跳转到下一页,结束爬虫')\n",
        "      break\n",
        "\n",
        "\n",
        "  result[keyword]=data\n",
        "#结束驱动\n",
        "browser.quit()\n",
        "\n",
        "#保存数据\n",
        "with open('carnegie{}.json'.format(start_time.split(' ')[0]),'w') as f:\n",
        "  f.write(json.dumps(result,ensure_ascii=False))\n",
        "\n",
        "print('————————————保存成功，完成爬取——————————————————')"
      ],
      "metadata": {
        "id": "zTCj_RFQYwjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}