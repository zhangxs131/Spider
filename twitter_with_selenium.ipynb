{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_with_selenium.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOne2F3pX27vwxI9/1y5ZEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhangxs131/Spider/blob/main/twitter_with_selenium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#twitter 爬取\n",
        "\n",
        "\n",
        "使用关键词爬取，twitter用户名，时间，正文，点赞，评论，转发数。\n",
        "\n",
        "使用selenium爬取twitter,没有使用API接口，但需要输入自己账号密码来设置cookie。\n",
        "\n",
        "可以设置\n",
        "\n",
        "*      关键词\n",
        "*      开始时间\n",
        "*      结束时间\n",
        "*      使用语言 \n",
        "*      最少点赞回复转发数目\n",
        "\n",
        "由于没有使用多线程和筛选条件，爬取速度较慢，也就1分钟80条左右"
      ],
      "metadata": {
        "id": "zNnnH8wTYRf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#安装selenium"
      ],
      "metadata": {
        "id": "Hsv8x6G_YSAY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5A_W2PbFXjmu",
        "outputId": "9a6cba18-344b-48f4-bf2f-1924e4a5934f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,628 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,478 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,067 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,256 kB]\n",
            "Fetched 9,685 kB in 3s (2,907 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "41 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 88.3 MB of archives.\n",
            "After this operation, 294 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 99.0.4844.51-0ubuntu0.18.04.1 [1,143 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 99.0.4844.51-0ubuntu0.18.04.1 [77.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 99.0.4844.51-0ubuntu0.18.04.1 [4,388 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 99.0.4844.51-0ubuntu0.18.04.1 [5,092 kB]\n",
            "Fetched 88.3 MB in 4s (23.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155335 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_99.0.4844.51-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
            "\u001b[K     |████████████████████████████████| 968 kB 2.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#selenium 设置"
      ],
      "metadata": {
        "id": "mR26-cFGYZKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "#配置option\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "\n",
        "browser= webdriver.Chrome(options=options)"
      ],
      "metadata": {
        "id": "L8Ub5W8DYfzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#登录twitter \n",
        "\n",
        "使用自己的账号密码\n",
        "username ******\n",
        "pwd    *******\n",
        "\n",
        "然后将cookie保存，便于后续使用"
      ],
      "metadata": {
        "id": "NTuRqQLzYiAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "#选择google搜索url\n",
        "browser.get('https://twitter.com/home')\n",
        "#选择器，使用NAME，选择查询框\n",
        "print(browser.title)\n",
        "\n",
        "user = '**********'\n",
        "pwd = '**********'\n",
        "\n",
        "# 填写邮件\n",
        "time.sleep(5)\n",
        "user_name = browser.find_element(by=By.XPATH,value='//div[@class=\"css-901oao r-1awozwy r-18jsvk2 r-6koalj r-37j5jr r-1inkyih r-16dba41 r-135wba7 r-bcqeeo r-13qz1uu r-qvutc0\"]/input')\n",
        "user_name.send_keys(user)\n",
        "user_name.send_keys(Keys.ENTER)\n",
        "\n",
        "# 填写密码\n",
        "time.sleep(5)\n",
        "password_input = browser.find_element(by=By.XPATH,value='//div[@class=\"css-901oao r-1awozwy r-18jsvk2 r-6koalj r-37j5jr r-1inkyih r-16dba41 r-135wba7 r-bcqeeo r-13qz1uu r-qvutc0\"]/input')\n",
        "password_input.send_keys(pwd)\n",
        "\n",
        "\n",
        "# 登陆\n",
        "password_input.send_keys(Keys.ENTER)\n",
        "time.sleep(5)\n",
        "cookies = browser.get_cookies()\n",
        "# print(cookies)\n",
        "with open('cookies-twitter.json', 'w') as f:\n",
        "    self_cookies = f.write(json.dumps(cookies))\n",
        "\n",
        "print(browser.title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neZTxMusYrrB",
        "outputId": "5e1b9609-ce03-422c-cd4f-f77c7a1e2e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twitter\n",
            "Twitter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#选择爬取内容\n",
        "\n",
        "*     用户名称\n",
        "*     twitter内容\n",
        "*     转发，评论，点赞量\n",
        "*     发布时间\n",
        "*     \n"
      ],
      "metadata": {
        "id": "wG_Z9UWNFY66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#调用Twitter高级搜索\n",
        "\n",
        "*         关键词\n",
        "*         使用语言\n",
        "*         最少点赞，回复，转发数目\n",
        "*         最早时间\n",
        "*         最晚时间"
      ],
      "metadata": {
        "id": "3QC9szNHEKH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#设置搜索参数\n",
        "keywords=['Ukraine','Russia']\n",
        "\n",
        "min_replies=1 #最少回复数\n",
        "min_likes=1  #最少点赞数\n",
        "min_retweet=0 #最小转发数\n",
        "\n",
        "start_time='2022-03-07'\n",
        "until_time='2022-03-10'\n",
        "\n",
        "language=['en','ru','fr','ja','ko','de','zh-cn','uk']  #表示 0英语 ，1俄语，2法语, 3日语，4法语, 5德语, 6简体中文 ，7乌克兰语\n",
        "lang=language[0]   #选择其中一种语言\n",
        "\n",
        "url='https://twitter.com/search?q={}%20min_replies%3A{}%20min_faves%3A{}%20min_retweets%3A{}%20lang%3A{}%20since%3A{}%20until%3A{}%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=top'\n",
        "deadline=5 #5次没有新增加信息则停止爬虫运行\n",
        "\n",
        "roll_num =10 #10000  # 设置鼠标最大滚动次数"
      ],
      "metadata": {
        "id": "M3iL48cGEGR_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "#配置option\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "browser= webdriver.Chrome(options=options)\n",
        "\n",
        "import time\n",
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "from selenium.webdriver.support.wait import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "\n",
        "# 读cookies\n",
        "cookies = []\n",
        "with open('cookies-twitter.json', 'r') as f:\n",
        "    cookies = json.loads(f.read())\n",
        "\n",
        "#设置twitter的cookie\n",
        "browser.get(\"https://twitter.com\")\n",
        "\n",
        "for cookie in cookies:\n",
        "    if 'expiry' in cookie:\n",
        "        del cookie['expiry']  # expiry Cookie有效终止日期\n",
        "    browser.add_cookie(cookie)\n",
        "\n",
        "\n",
        "#要爬取的url\n",
        "spider_url=url.format(('%20').join(keywords),min_replies,min_likes,min_retweet,lang,start_time,until_time)\n",
        "print(spider_url)\n",
        "browser.get(spider_url)\n",
        "\n",
        "time.sleep(2)\n",
        "print('页面标题',browser.title)\n",
        "\n",
        "\n",
        "#设置last模型搜索\n",
        "WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//a[@class=\"css-4rbku5 css-18t94o4 css-1dbjc4n r-1awozwy r-1loqt21 r-6koalj r-eqz5dr r-16y2uox r-1h3ijdo r-1777fci r-s8bhmr r-1ny4l3l r-1qhn6m8 r-i023vh r-o7ynqc r-6416eg\"]')))\n",
        "last_mode=browser.find_elements(by=By.XPATH,value='//a[@class=\"css-4rbku5 css-18t94o4 css-1dbjc4n r-1awozwy r-1loqt21 r-6koalj r-eqz5dr r-16y2uox r-1h3ijdo r-1777fci r-s8bhmr r-1ny4l3l r-1qhn6m8 r-i023vh r-o7ynqc r-6416eg\"]')\n",
        "\n",
        "#last_mode 共5个模式 top ,last,people,photo,video 这里设置为时间倒序\n",
        "last_mode[1].click()\n",
        "\n",
        "data = []\n",
        "\n",
        "num = 0  # 推文总数\n",
        "last_twitter_time =until_time+' 00:00:00'\n",
        "\n",
        "for roll_i in range(roll_num):\n",
        "  last_num=len(data)\n",
        "\n",
        "  #滑动鼠标\n",
        "  js = 'window.scrollBy(0,document.body.scrollHeight)'\n",
        "  browser.execute_script(js) \n",
        "\n",
        "  print('上此检索的时间',last_twitter_time)\n",
        "  print(\"第 {} 次滑动鼠标\".format(roll_i+1)) \n",
        "\n",
        "  # try:\n",
        "  WebDriverWait(browser, 15).until(EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-kzbkwu\"]')))\n",
        "  lists=browser.find_elements(by=By.XPATH,value='//div[@class=\"css-1dbjc4n r-1iusvr4 r-16y2uox r-1777fci r-kzbkwu\"]')\n",
        "\n",
        "  #   #找到上次加载最后一个li\n",
        "\n",
        "  # if last_twitter in lists:\n",
        "  #   idx=list.index(last_twitter)\n",
        "  #   print('找到最后一个li')\n",
        "  # else:\n",
        "  #   idx=-1\n",
        "  \n",
        "  # lase_twitter=lists[-1]\n",
        "  # print('增加{}条twitter'.format(len(lists)-idx-1))\n",
        "  # if len(lists)-idx-1<1:\n",
        "  #   break\n",
        "\n",
        "  sample={}\n",
        "  for i in lists:\n",
        "    try:\n",
        "      t1=i.find_elements(by=By.XPATH,value='./div')\n",
        "      t2=t1[1].find_elements(by=By.XPATH,value='./div')\n",
        "\n",
        "      this_time=t1[0].find_element(by=By.XPATH,value='.//time').get_attribute('datetime')\n",
        "      sample['time']=this_time[:-5].replace('T',' ')\n",
        "\n",
        "      #通过时间检测是否，已经遍历过\n",
        "      if sample['time']>=last_twitter_time:\n",
        "        continue\n",
        "\n",
        "      #用户名\n",
        "      sample['username']=t1[0].find_element(by=By.XPATH,value='.//span[@class=\"css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0\"]').text\n",
        "\n",
        "      text_div=t2[0].find_elements(by=By.XPATH,value='.//span')\n",
        "      #文本\n",
        "      sample['text']=('').join([txt.text for txt in text_div]) \n",
        "      like_div=t2[2].find_elements(by=By.XPATH,value='./div/div')\n",
        "      #点赞，评论数\n",
        "      likes=[]\n",
        "      for j in like_div[:3]:\n",
        "        likes.append(j.find_element(by=By.XPATH,value='.//span[@class=\"css-901oao css-16my406 r-poiln3 r-bcqeeo r-qvutc0\"]').text)\n",
        "      sample['comments']=likes[0]\n",
        "      sample['forwards']=likes[1]\n",
        "      sample['likes']=likes[2]    \n",
        "      #print(sample)\n",
        "      data.append(sample)\n",
        "      num+=1\n",
        "      if num%10==0:\n",
        "        print('第 {} 条数据\\n {}'.format(num,sample))\n",
        "    except:\n",
        "      continue\n",
        "  #设置上次最后的时间\n",
        "  last_twitter_time=sample['time']\n",
        "  if len(data)==last_num:\n",
        "    deadline-=1\n",
        "    if deadline==0:\n",
        "      print('无额外信息')\n",
        "      break\n",
        "with open('twitter_{}_{}_{}.json'.format(lang,start_time,until_time), 'w', encoding='utf-8') as f:\n",
        "  f.write(json.dumps(data,ensure_ascii=False))\n",
        "print('————————爬取结束，共{}条数据'.format(num))"
      ],
      "metadata": {
        "id": "BeFbuSt_eOxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9f7f19-94c8-484f-b4b8-a5fb734c530a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://twitter.com/search?q=Ukraine%20Russia%20min_replies%3A1%20min_faves%3A1%20min_retweets%3A0%20lang%3Aen%20since%3A2022-03-07%20until%3A2022-03-10%20-filter%3Alinks%20-filter%3Areplies&src=typed_query&f=top\n",
            "页面标题 Ukraine Russia min_replies:1 min_faves:1 min_retweets:0 lang:en since:2022-03-07 until:2022-03-10 -filter:links -filter:replies - Twitter Search / Twitter\n",
            "上此检索的时间 2022-03-10 00:00:00\n",
            "第 1 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:56:18\n",
            "第 2 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:48:20\n",
            "第 3 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:38:45\n",
            "第 4 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:30:53\n",
            "第 5 次滑动鼠标\n",
            "第 10 条数据\n",
            " {'time': '2022-03-09 23:23:09', 'username': 'Rob Jamieson', 'text': 'The inference that Russia deliberately targeted a kids hospital. Really. Are any of you thick enough to believe that?\\n\\nWish Ukraine would just vanish off the map. Their constant whining and bullshit is just taking the piss.', 'comments': '2', 'forwards': '3', 'likes': '2'}\n",
            "上此检索的时间 2022-03-09 23:23:09\n",
            "第 6 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:14:24\n",
            "第 7 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 23:08:27\n",
            "第 8 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 22:57:21\n",
            "第 9 次滑动鼠标\n",
            "上此检索的时间 2022-03-09 22:50:50\n",
            "第 10 次滑动鼠标\n",
            "————————爬取结束，共18条数据\n"
          ]
        }
      ]
    }
  ]
}